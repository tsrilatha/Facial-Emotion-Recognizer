# -*- coding: utf-8 -*-
"""EE258_NN_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m_UfMZVuOWZna4miBe9EQg-CcvshuAk7
"""

# IMport required libraries
import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from random import randint
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from tensorflow.keras import optimizers
from tensorflow.keras.models import Model
from keras.layers import Activation, Dense
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.layers import Flatten, Dense, GlobalAvgPool2D, GlobalMaxPool2D
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import plot_model
from keras.utils import np_utils

#To mount Google Drive on runtime to read and write the files in the drive.
from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
#import required libraries
# %cd /content/gdrive/MyDrive/EE258_NN
# function to load images and respective labels
def load_img(directory):
  Images = []
  Labels = []
  for labels in os.listdir(directory):
    if labels == 'angry':
      label = 0
    elif labels == 'disgust':
      label = 1
    elif labels == 'fear':
      label = 2
    elif labels == 'happy':
      label = 3
    elif labels == 'neutral':
      label = 4
    elif labels == 'sad':
      label = 5
    elif labels == 'surprise':
      label = 6
    for image_file in os.listdir(directory+labels):
      image = cv2.imread(directory+labels+'/'+image_file, cv2.IMREAD_GRAYSCALE) #  add labels to the images which is in gray scale.
      image = cv2.resize(image, (48,48))
      #image /= 255 Normalise
      Images.append(image)
      Labels.append(label)
  return shuffle(Images, Labels, random_state = 40)

def get_classlabel(class_code):
  labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'neutral',5:'sad',6:'surprise'}
  return labels[class_code]

# load the images from the gdrive folder seperately for test and train
train_Img_data, train_Label_data = load_img('/content/gdrive/MyDrive/EE258_NN/train/')
test_Img_data, test_Label_data = load_img('/content/gdrive/MyDrive/EE258_NN/test/')

## Saving the data as numpy array for faster reloads
train_Img_np = np.array(train_Img_data)
train_Label_np = np.array(train_Label_data)
np.save('/content/gdrive/MyDrive/EE258_NN/train_new/train_Images.npy',train_Img_np)
np.save('/content/gdrive/MyDrive/EE258_NN/train_new/train_Labels.npy',train_Label_np)
test_Img_np = np.array(test_Img_data)
test_Label_np = np.array(test_Label_data)
np.save('/content/gdrive/MyDrive/EE258_NN/test_new/test_Images.npy',test_Img_np)
np.save('/content/gdrive/MyDrive/EE258_NN/test_new/test_Labels.npy',test_Label_np)

## Loading the data from numpy array
train_loaded_images = np.load('/content/gdrive/MyDrive/EE258_NN/train_new/train_Images.npy')
train_loaded_labels = np.load('/content/gdrive/MyDrive/EE258_NN/train_new/train_Labels.npy')
test_loaded_images = np.load('/content/gdrive/MyDrive/EE258_NN/test_new/test_Images.npy')
test_loaded_labels = np.load('/content/gdrive/MyDrive/EE258_NN/test_new/test_Labels.npy')

# Plotting random images from the train dataset
fig,ax = plt.subplots(1,2)
for i in range(1):
  rnd_number = randint(0,len(train_loaded_labels))
  ax[i].imshow(train_loaded_images[rnd_number],cmap='gray')
  ax[i].set_title(get_classlabel(train_loaded_labels[rnd_number]))
  label_values, count = np.unique(train_loaded_labels,return_counts = True)
  distribution = dict(zip(label_values, count))
  plt.bar(list(distribution.keys()), distribution.values(),width = 0.6)
  plt.xlabel('Image Labels')
  plt.ylabel('Count')
  plt.show
print(distribution) #Print how many imgaes are there in the respective classes
print(train_loaded_images.shape) # Total number of images and image shape
print(train_loaded_labels.shape) # Number of labels

#Print the images and labels saved as np.array
print(train_loaded_labels)
print(train_loaded_images)

# Plotting random multiple images from the train dataset
fig,ax = plt.subplots(1,6)
#print(ax)
for i in range(6):
  rnd_number = randint(0,len(train_loaded_labels))
  ax[i].imshow(train_loaded_images[rnd_number],cmap='gray')
  ax[i].set_title(get_classlabel(train_loaded_labels[rnd_number]))
  label_values, count = np.unique(train_loaded_labels,return_counts = True)
  distribution = dict(zip(label_values, count))
  plt.show

# Plotting random images from the test dataset
fig,ax = plt.subplots(1,2)
for i in range(1):
  rnd_number = randint(0,len(test_loaded_labels))
  ax[i].imshow(test_loaded_images[rnd_number],cmap='gray')
  ax[i].set_title(get_classlabel(test_loaded_labels[rnd_number]))
  label_values, count = np.unique(test_loaded_labels,return_counts = True)
  distribution = dict(zip(label_values, count))
  plt.bar(list(distribution.keys()), distribution.values(),width = 0.6)
  plt.xlabel('Image Labels')
  plt.ylabel('Count')
  plt.show
print(distribution) #Print how many imgaes are there in the respective classes
print(test_loaded_images.shape) # Total number of images and image shape
print(test_loaded_labels.shape) # Number of labels

# Plotting random multiple images from the test dataset
fig,ax = plt.subplots(1,6)
for i in range(6):
  rnd_number = randint(0,len(test_loaded_labels))
  ax[i].imshow(test_loaded_images[rnd_number],cmap='gray')
  ax[i].set_title(get_classlabel(test_loaded_labels[rnd_number]))
  plt.show

#Print the images and labels saved as np.array
print(test_loaded_labels)
print(test_loaded_images)

#Split the training data  into train and validation.
x_train_data, x_val_data, y_train_data, y_val_data  = train_test_split(train_loaded_images,train_loaded_labels, test_size = 1/12, random_state = 42)
print(x_train_data.shape, x_val_data.shape, y_train_data.shape, y_val_data.shape)

# reshaping and normalising the samples.
x_train_data_reshaped = x_train_data.reshape(len(x_train_data), 48*48*1)
x_val_data_reshaped = x_val_data.reshape(len(x_val_data), 48*48*1)
x_test_data_reshaped = test_loaded_images.reshape(len(test_loaded_images), 48*48*1)

X_train = x_train_data_reshaped.astype('float32')/255.0
y_train = y_train_data.astype('uint')
X_val = x_val_data_reshaped.astype('float32')/255.0
y_val = y_val_data.astype('uint')
X_test = x_test_data_reshaped.astype('float32')/255.0
y_test = test_loaded_labels.astype('uint')

# Creating a MLP model . Model 1
model1 = keras.models.Sequential()
model1.add(keras.layers.Dense(300, activation ="relu"))
model1.add(keras.layers.Dense(300, activation ="relu"))
model1.add(keras.layers.Dense(300, activation ="relu"))
model1.add(keras.layers.Dense(100, activation ="relu"))
model1.add(keras.layers.Dropout(0.25))
model1.add(keras.layers.Dense(7, activation = "softmax"))
sgd = SGD(learning_rate=0.001)
model1.compile(optimizer=sgd,   loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the MLP network
model1.fit(X_train, y_train, batch_size=10, epochs=16, validation_data =(X_val, y_val))

# Plot the training loss and accuracy graph for model1
data = model1.history.history
pd.DataFrame(data).plot(figsize=(8, 5))
plt.grid(True)
plt.show()

# Displaying model summary and test accuracy.
model1.summary()
loss1_train, acc1_train = model1.evaluate(X_train, y_train, verbose=0)
print('Accuracy: %.3f' % acc1_train)
loss1_test, acc1_test = model1.evaluate(X_test, y_test, verbose=0)
print('Accuracy: %.3f' % acc1_test)

# Confusion matrix for model 1
y_proba1 = model1.predict(X_test)
y_pred1 = np.argmax(y_proba1,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred1, y_test))
print('Classification Report')
print(classification_report(y_pred1, y_test))

# normalising the samples without reshaping.
X_train_norm = x_train_data.astype('float32')/255.0
y_train_norm= y_train_data.astype('uint')
X_val_norm = x_val_data.astype('float32')/255.0
y_val_norm = y_val_data.astype('uint')
X_test_norm = test_loaded_images.astype('float32')/255.0
y_test_norm = test_loaded_labels.astype('uint')

# Creating a CNN model. Model 2
model2 = keras.models.Sequential()
model2.add(Conv2D(32, kernel_size = (3,3), input_shape = (48,48,1) ,activation = 'relu'))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(Conv2D(64, (3,3), activation = 'relu', padding = 'same'))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(Conv2D(128, (3,3), activation = 'relu', padding = 'same'))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(Conv2D(256, (3,3), activation = 'relu', padding = 'same'))
model2.add(MaxPooling2D(pool_size=(2,2)))
model2.add(keras.layers.Dropout(0.25))
model2.add(keras.layers.Flatten())
model2.add(Dense(7, activation = 'softmax'))
model2.summary()
model2.compile(optimizer = 'Adam',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])
model2.fit(X_train_norm,y_train_norm, batch_size=10, epochs = 16, validation_data=(X_val_norm,y_val_norm))
# Plot the training loss and accuracy graph for model2
pd.DataFrame(model2.history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.show()
!mkdir -p saved_model
model2.save('/content/gdrive/MyDrive/EE258_NN/train_new/cnn_v2')

# obtaining train and test accuracy
loss2_train, acc2_train = model2.evaluate(X_train_norm, y_train_norm, verbose=1)
print('Accuracy: %.3f' % acc2_train)
loss2_test, acc2_test = model2.evaluate(X_test_norm, y_test_norm, verbose=1)
print('Accuracy: %.3f' % acc2_test)

# obtaining the confusion matrix for model 2
y_proba2 = model2.predict(X_test_norm)
y_pred2 = np.argmax(y_proba2,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred2, y_test_norm))
print('Classification Report')
print(classification_report(y_pred2, y_test_norm))

#Creating a CNN model. Model 3.
model3 = keras.models.Sequential()
model3.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape= (48,48,1), data_format='channels_last', kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))
model3.add(MaxPooling2D(pool_size=(2,2)))
model3.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
model3.add(MaxPooling2D(pool_size=(2, 2)))
model3.add(keras.layers.Dropout(0.5))
model3.add(Conv2D(2*64, kernel_size=(3, 3), activation='relu', padding='same'))
model3.add(MaxPooling2D(pool_size=(2, 2)))
model3.add(keras.layers.Dropout(0.5))
model3.add(Dense(300, activation='relu'))
model3.add(keras.layers.Dropout(0.25))
model3.add(keras.layers.Flatten())
model3.add(Dense(7, activation='softmax'))
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model3.summary()
model3.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',metrics = ['accuracy'])
model3.fit(X_train_norm,y_train_norm, batch_size=10, epochs = 16, validation_data=(X_val_norm,y_val_norm))
# Plot the training loss and accuracy graph for model3
pd.DataFrame(model3.history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.show()
!mkdir -p saved_model
model3.save('/content/gdrive/MyDrive/EE258_NN/train_new/cnn_v3')

# obtaining train and test accuracy
loss3_train, acc3_train = model3.evaluate(X_train_norm, y_train_norm, verbose=1)
print('Accuracy: %.3f' % acc3_train)
loss3_test, acc3_test = model3.evaluate(X_test_norm, y_test_norm, verbose=1)
print('Accuracy: %.3f' % acc3_test)

# obtaining the confusion matrix for model 3
y_proba3 = model3.predict(X_test_norm)
y_pred3 = np.argmax(y_proba3,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred3, y_test_norm))
print('Classification Report')
print(classification_report(y_pred3, y_test_norm))

EPOCHS = 10
BS = 32
# construct the training image generator for data augmentation
aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,
	width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,
	horizontal_flip=True, fill_mode="nearest")
# train the network
X_train_norm_reshape = X_train_norm.reshape(-1, 48, 48, 1)
X_val_norm_reshape = X_val_norm.reshape(-1, 48, 48, 1)
X_test_norm_reshape= X_test_norm.reshape(-1, 48, 48, 1)
# using the data augmetation on the model3
model3.fit_generator(aug.flow(X_train_norm_reshape, y_train_norm, batch_size=BS),
	validation_data=(X_val_norm_reshape, y_val_norm), steps_per_epoch=len(X_train_norm_reshape) // BS,
	epochs=EPOCHS)

# Summarize the model3.
model3.summary()

# Plot the training loss and accuracy graph for model3 with augmentation
pd.DataFrame(model_history).plot(figsize=(8, 5))
plt.grid(True)
plt.show()

# obtaining train and test accuracy for the data augmented model3
X_test_norm_reshape= X_test_norm.reshape(-1, 48, 48, 1)
loss3_train, acc3_train = model3.evaluate(X_train_norm_reshape, y_train_norm, verbose=1)
print('Accuracy: %.3f' % acc3_train)
loss3_test, acc3_test = model3.evaluate(X_test_norm_reshape, y_test_norm, verbose=1)
print('Accuracy: %.3f' % acc3_test)

# obtaining the confusion matrix for model 3 with data augumentation
y_proba3 = model3.predict(X_test_norm)
y_pred3 = np.argmax(y_proba3,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred3, y_test_norm))
print('Classification Report')
print(classification_report(y_pred3, y_test_norm))

# Early stopping. Model3
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
model3.fit(X_train_norm,y_train_norm, batch_size=10, epochs = 30, callbacks=[callback], validation_data=(X_val_norm,y_val_norm))

# obtaining train and test accuracy for the data augmented model3
loss3_train, acc3_train = model3.evaluate(X_train_norm_reshape, y_train_norm, verbose=1)
print('Accuracy: %.3f' % acc3_train)
loss3_test, acc3_test = model3.evaluate(X_test_norm_reshape, y_test_norm, verbose=1)
print('Accuracy: %.3f' % acc3_test)

# obtaining the confusion matrix for model 3 with data augumentation
y_proba3 = model3.predict(X_test_norm)
y_pred3 = np.argmax(y_proba3,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred3, y_test_norm))
print('Classification Report')
print(classification_report(y_pred3, y_test_norm))

#Used for pretrained Model
X_train_color = np.repeat((x_train_data.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
X_valid_color = np.repeat((x_val_data.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
y_train_color = np_utils.to_categorical(y_train)
y_valid_color = np_utils.to_categorical(y_val)
print(X_train_color.shape)
print(X_valid_color.shape)
print(y_train_color.shape)
print(y_valid_color.shape)

# Using vgg16 as the pre trained model.
base_model = tf.keras.applications.VGG16(
    input_shape = (48, 48, 3),
    include_top = False,
    weights = 'imagenet'
)
#Start with a non trainable model
base_model.trainable = False
tf.keras.utils.plot_model(base_model, show_shapes=True)

# Adding few fully connected layers to the pre trained model to get our classified output.
np.random.seed(7)
tf.random.set_seed(7)
model_4 = keras.models.Sequential ([
    base_model,
    keras.layers.Flatten(),
    Dense(64, activation='relu'),
    keras.layers.Dropout(0.50),
    Dense(32, activation='relu'),
    keras.layers.Dropout(0.25),
    Dense(32, activation='relu'),
    keras.layers.Dropout(0.15),
    Dense(7, activation='softmax')
])
model_4.summary()

# compiling the model_4
#opt = tf.keras.optimizers.Adam(0.0001)
model_4.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

#training the network model_4
h1 = model_4.fit(
    X_train_color,y_train_color,
    steps_per_epoch=len(X_train_color) / 64,
    epochs=10,
    validation_data=(X_valid_color,y_valid_color),
    validation_steps=len(X_valid_color) / 64,
    verbose=1
)

X_test_color=np.repeat((test_loaded_images.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
y_test_color = np_utils.to_categorical(y_test)
data = model_4.history.history
pd.DataFrame(data).plot(figsize=(8, 5))
plt.grid(True)
plt.show()
y_proba4 = model_4.predict(X_test_color)
y_pred4 = np.argmax(y_proba3,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred4, y_test))
print('Classification Report')
print(classification_report(y_pred4, y_test))

# implement the  trainable model
base_model.trainable = True
opt = tf.keras.optimizers.Adam(0.00001)
model_4.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
h2 = model_4.fit(
    X_train_color,y_train_color,
    steps_per_epoch=len(X_train_color) / 64,
    epochs=10,
    validation_data=(X_valid_color,y_valid_color),
    validation_steps=len(X_valid_color) / 64,
    verbose=1
)

model_4.summary()

data = model_4.history.history
pd.DataFrame(data).plot(figsize=(8, 5))
plt.grid(True)
plt.show()

y_proba4 = model_4.predict(X_test_color)
y_pred4 = np.argmax(y_proba4,axis=1)
print('Confusion Matrix')
print(confusion_matrix(y_pred4, y_test))
print('Classification Report')
print(classification_report(y_pred4, y_test))

# obtaining train and test accuracy for the pretrained model
X_test_color=np.repeat((test_loaded_images.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
y_test_color = np_utils.to_categorical(y_test)
print(X_test_color.shape)
loss4_train, acc4_train = model_4.evaluate(X_train_color, y_train_color, verbose=1)
print('Accuracy: %.3f' % acc4_train)
loss4_test, acc4_test = model_4.evaluate(X_test_color, y_test_color, verbose=1)
print('Accuracy: %.3f' % acc4_test)

#Used for pretrained Model
X_train_color = np.repeat((x_train_data.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
X_valid_color = np.repeat((x_val_data.astype('float32')/255.0).reshape(-1, 48, 48, 1), repeats=3, axis=3)
y_train_color = np_utils.to_categorical(y_train)
y_valid_color = np_utils.to_categorical(y_val)
print(X_train_color.shape)
print(X_valid_color.shape)
print(y_train_color.shape)
print(y_valid_color.shape)

#Creating a CNN model. Model 5 for ensembing.
model_5 = keras.models.Sequential()
model_5.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape= (48,48,3), data_format='channels_last', kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)))
model_5.add(MaxPooling2D(pool_size=(2,2)))
model_5.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
model_5.add(MaxPooling2D(pool_size=(2, 2)))
model_5.add(keras.layers.Dropout(0.5))
model_5.add(Conv2D(2*64, kernel_size=(3, 3), activation='relu', padding='same'))
model_5.add(MaxPooling2D(pool_size=(2, 2)))
model_5.add(keras.layers.Dropout(0.5))
model_5.add(Dense(300, activation='relu'))
model_5.add(keras.layers.Dropout(0.25))
model_5.add(keras.layers.Flatten())
model_5.add(Dense(7, activation='softmax'))
model_5.summary()
opt = tf.keras.optimizers.Adam(0.00001)
model_5.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])
model_5.fit(X_train_color,y_train_color, batch_size=10, epochs = 24, validation_data=(X_valid_color,y_valid_color))

# implement the  trainable model
# Adding few fully connected layers to the pre trained model to get our classified output.
# Using vgg16 as the pre trained model.
base_model = tf.keras.applications.VGG16(
    input_shape = (48, 48, 3),
    include_top = False,
    weights = 'imagenet'
)

tf.keras.utils.plot_model(base_model, show_shapes=True)

np.random.seed(7)
tf.random.set_seed(7)
model_6 = keras.models.Sequential ([
    base_model,
    keras.layers.Flatten(),
    Dense(128, activation='relu'),
    keras.layers.Dropout(0.50),
    Dense(64, activation='relu'),
    keras.layers.Dropout(0.25),
    Dense(32, activation='relu'),
    keras.layers.Dropout(0.15),
    Dense(7, activation='softmax')
])
model_6.summary()

base_model.trainable = True
opt = tf.keras.optimizers.Adam(0.00001)
model_6.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
h2 = model_6.fit(
    X_train_color,y_train_color,
    steps_per_epoch=len(X_train_color) / 64,
    epochs=10,
    validation_data=(X_valid_color,y_valid_color),
    validation_steps=len(X_valid_color) / 64,
    verbose=1
)

class WeightedAverageLayer(tf.keras.layers.Layer):
    def __init__(self, w1, w2, **kwargs):
        super(WeightedAverageLayer, self).__init__(**kwargs)
        self.w1 = w1
        self.w2 = w2

    def call(self, inputs):
        return self.w1 * inputs[0] + self.w2 * inputs[1]

# combining 2 models for ensembling
from tensorflow.keras.layers import Input, Average
models = [model_4, model_5]
model_input = Input(shape=(48, 48, 3))
model_outputs = [model(model_input) for model in models]
ensemble_output = WeightedAverageLayer(0.6, 0.4)(model_outputs)
ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')

# train the emsemble network
ensemble_model.summary()
opt = tf.keras.optimizers.Adam(0.00001)

ensemble_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])
ensemble_model.fit(X_train_color,y_train_color, batch_size=10, epochs = 24, validation_data=(X_valid_color,y_valid_color))

#obtain the val and test accuracy for ensemble model
print(X_test_color.shape)
loss5_train, acc5_train = ensemble_model.evaluate(X_train_color, y_train_color, verbose=1)
print('Accuracy: %.3f' % acc5_train)
loss5_test, acc5_test = ensemble_model.evaluate(X_test_color, y_test_color, verbose=1)
print('Accuracy: %.3f' % acc5_test)

